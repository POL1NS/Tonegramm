import sys
import subprocess
import pkg_resources
import importlib.util
import re
import json
import asyncio
from collections import defaultdict
import signal
import time

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è
class GracefulExiter:
    def __init__(self):
        self.state = False
        signal.signal(signal.SIGINT, self.change_state)

    def change_state(self, signum, frame):
        print("\n–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è, –∑–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É...")
        self.state = True

    def exit(self):
        return self.state

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è
flag = GracefulExiter()

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–∫–µ—Ç–æ–≤
required = {'telethon', 'nest_asyncio', 'pymorphy3', 'transformers', 'torch', 'pandas', 'plotly', 'nltk'}
installed = {pkg.key for pkg in pkg_resources.working_set}
missing = required - installed

if missing:
    print(f"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–∫–µ—Ç–æ–≤: {missing}")
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)
        print("–ü–∞–∫–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")
    except subprocess.CalledProcessError as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ: {e}. –ü–æ–ø—ã—Ç–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Å --user")
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--user', *missing], stdout=subprocess.DEVNULL)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è telethon
if importlib.util.find_spec("telethon") is None:
    raise ModuleNotFoundError("–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å telethon. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—Ä—É—á–Ω—É—é: pip install telethon")

# –ò–º–ø–æ—Ä—Ç –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥—É–ª–µ–π –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
from telethon.sync import TelegramClient
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import pandas as pd
import pymorphy3
import plotly.express as px
import plotly.io as pio
import nest_asyncio
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
nest_asyncio.apply()
nltk.download('vader_lexicon')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API Telegram
api_id = '–∞–π–ø–∏ –∞–π–¥–∏'
api_hash = "–∞–π–ø–∏ —Ö–∞—à"
phone = "–Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞"
channels = ['—Å—Å—ã–ª–∫–∞ –Ω–∞ –∫–∞–Ω–∞–ª']

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
ru_model_name = "blanchefort/rubert-base-cased-sentiment"
ru_tokenizer = AutoTokenizer.from_pretrained(ru_model_name, model_max_length=512)
ru_model = AutoModelForSequenceClassification.from_pretrained(ru_model_name)

# –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
morph = pymorphy3.MorphAnalyzer()

def detect_language(text):
    if re.search(r'[–∞-—è–ê-–Ø]', text):
        return 'ru'
    elif re.search(r'[a-zA-Z]', text):
        return 'en'
    return 'unknown'

def get_word_forms(word):
    try:
        parsed = morph.parse(word)[0]
        return set([w.word for w in parsed.lexeme])
    except:
        return {word}

def preprocess_text(text):
    emoji_map = {
        'üî•': ' –æ—Ç–ª–∏—á–Ω–æ ', '‚ù§Ô∏è': ' –ª—é–±–æ–≤—å ', '‚≠êÔ∏è': ' –∑–≤–µ–∑–¥–∞ ', '‚ú®': ' –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ ',
        'üëç': ' –æ–¥–æ–±—Ä–µ–Ω–∏–µ ', 'üéâ': ' –ø—Ä–∞–∑–¥–Ω–∏–∫ ', 'üòä': ' —É–ª—ã–±–∫–∞ ', 'üòÇ': ' —Å–º–µ—Ö ',
        'üòç': ' –æ–±–æ–∂–∞–Ω–∏–µ ', 'üôè': ' —Å–ø–∞—Å–∏–±–æ ', 'ü§ó': ' –æ–±—ä—è—Ç–∏—è ', 'üò¢': ' –≥—Ä—É—Å—Ç—å ',
        'üò°': ' –∑–ª–æ—Å—Ç—å ', 'ü§î': ' –∑–∞–¥—É–º—á–∏–≤–æ—Å—Ç—å ', 'üëè': ' –∞–ø–ª–æ–¥–∏—Å–º–µ–Ω—Ç—ã '
    }
    for emoji, replacement in emoji_map.items():
        text = text.replace(emoji, replacement)
    return text

def get_sentiment_category(overall_score, text=None, threshold=30):  # –£–≤–µ–ª–∏—á–∏–ª –ø–æ—Ä–æ–≥
    if not text:
        if overall_score >= threshold:
            return "–ü–æ–∑–∏—Ç–∏–≤–Ω–æ"
        elif overall_score <= -threshold:
            return "–ù–µ–≥–∞—Ç–∏–≤–Ω–æ"
        return "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ"
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –º–∞—Ä–∫–µ—Ä–æ–≤
    positive_markers = ['‚ù§Ô∏è', 'üî•', '–º–æ–ª–æ–¥—Ü—ã', '–ø–æ–∑–¥—Ä–∞–≤–ª—è', '–æ—Ç–ª–∏—á–Ω', '—É–º–Ω–∏', 
                       '—Å—É–ø–µ—Ä', '—Å–ø–∞—Å–∏–±–æ', '–±–ª–∞–≥–æ–¥–∞—Ä—é', '–∫–ª–∞—Å—Å', '–∫—Ä—É—Ç–æ', 
                       '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '–≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ', '—Ä–∞–¥', '—Å—á–∞—Å—Ç—å–µ']
    
    negative_markers = ['–ø–ª–æ—Ö–æ', '—É–∂–∞—Å–Ω–æ', '–∫–æ—à–º–∞—Ä', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω', '–∑–ª—é—Å—å', 
                       '–Ω–µ–Ω–∞–≤–∏–∂—É', '–±–µ—Å–∏—Ç', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ', '–≥—Ä—É—Å—Ç–Ω–æ', '–æ–±–∏–¥–Ω–æ']
    
    neutral_markers = ['–Ω–µ ', '–Ω–µ—Ç', '–Ω–æ ', '–±–µ–∑ ', '–Ω–∏–∫—Ç–æ', '–Ω–∏–∫–æ–≥–¥–∞', '–ø—Ä–æ–±–ª–µ–º–∞',
                      '–æ—à–∏–±–∫–∞', '–∏—Å–ø—Ä–∞–≤—å—Ç–µ', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ']
    
    # –ü–æ–¥—Å—á–µ—Ç –º–∞—Ä–∫–µ—Ä–æ–≤
    text_lower = text.lower()
    pos_count = sum(1 for m in positive_markers if m in text_lower)
    neg_count = sum(1 for m in negative_markers if m in text_lower)
    neu_count = sum(1 for m in neutral_markers if m in text_lower)
    
    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –æ—Ü–µ–Ω–∫–∏
    adjusted_score = overall_score + (pos_count * 8) - (neg_count * 10) - (neu_count * 3)
    
    if adjusted_score >= threshold or pos_count >= 3:
        return "–ü–æ–∑–∏—Ç–∏–≤–Ω–æ"
    elif adjusted_score <= -threshold or neg_count >= 2:
        return "–ù–µ–≥–∞—Ç–∏–≤–Ω–æ"
    return "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ"

async def analyze_channel_async(client, channel_url):
    results = []
    conflict_cases = [] 
    user_stats = defaultdict(lambda: {'count': 0, 'total_score': 0, 'positive': 0, 'negative': 0})
    
    try:
        target = await client.get_entity(channel_url)
        channel_name = getattr(target, 'title', 'Unknown Channel')
        
        messages = await client.get_messages(target, limit=1000)  # –£–º–µ–Ω—å—à–∏–ª –ª–∏–º–∏—Ç
        
        for i, msg in enumerate(messages):
            if flag.exit():
                print(f"–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞ {channel_name}")
                break
                
            if not msg.text:
                continue
                
            try:
                lang = detect_language(msg.text)
                raw_text = msg.text[:10000]  # –£–º–µ–Ω—å—à–∏–ª –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
                text = preprocess_text(raw_text)
                
                if lang == 'ru':
                    inputs = ru_tokenizer(
                        text, 
                        return_tensors='pt', 
                        truncation=True, 
                        padding=True,
                        max_length=256  # –£–º–µ–Ω—å—à–∏–ª –¥–ª–∏–Ω—É
                    )
                    with torch.no_grad():
                        outputs = ru_model(**inputs)
                        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze().tolist()
                    
                    if ru_model.config.id2label[0] == "negative":
                        neg, neu, pos = probs
                    else:
                        label_to_index = {'neg': None, 'neu': None, 'pos': None}
                        for i in range(3):
                            label = ru_model.config.id2label[i].lower()
                            if 'neg' in label:
                                label_to_index['neg'] = i
                            elif 'pos' in label:
                                label_to_index['pos'] = i
                            else:
                                label_to_index['neu'] = i
                        neg = probs[label_to_index['neg']]
                        neu = probs[label_to_index['neu']]
                        pos = probs[label_to_index['pos']]
                    
                    neg_percent = neg * 100
                    neu_percent = neu * 100
                    pos_percent = pos * 100
                    overall_score = pos_percent - neg_percent
                    sentiment_category = get_sentiment_category(overall_score, raw_text)
                    
                    # –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
                    username = '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π'
                    if hasattr(msg, 'sender') and msg.sender:
                        username = getattr(msg.sender, 'username', 
                                         getattr(msg.sender, 'first_name', 
                                                str(getattr(msg.sender, 'id', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π'))))

                    user_stats[username]['count'] += 1
                    user_stats[username]['total_score'] += overall_score
                    if sentiment_category == "–ü–æ–∑–∏—Ç–∏–≤–Ω–æ":
                        user_stats[username]['positive'] += 1
                    elif sentiment_category == "–ù–µ–≥–∞—Ç–∏–≤–Ω–æ":
                        user_stats[username]['negative'] += 1
                
                    results.append({
                        '–î–∞—Ç–∞': msg.date.strftime('%Y-%m-%d %H:%M:%S'),
                        '–¢–µ–∫—Å—Ç': raw_text[:300] + "..." if len(raw_text) > 300 else raw_text,
                        '–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç': raw_text,
                        '–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç': text,
                        '–ù–µ–≥–∞—Ç–∏–≤–Ω–æ': round(neg_percent, 1),
                        '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ': round(neu_percent, 1),
                        '–ü–æ–∑–∏—Ç–∏–≤–Ω–æ': round(pos_percent, 1),
                        '–û–±—â–∏–π –±–∞–ª–ª': round(overall_score, 3),
                        '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': sentiment_category,
                        '–Ø–∑—ã–∫': lang,
                        '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å': username,
                        '–ö–∞–Ω–∞–ª': channel_name
                    })
                    
                    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                    if i % 50 == 0:
                        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i} —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∫–∞–Ω–∞–ª–∞ {channel_name}")
                        
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {str(e)}")
                continue
                
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–∞–Ω–∞–ª–∞ {channel_url}: {str(e)}")
    
    return results, conflict_cases, user_stats

async def analyze_messages_async():
    all_results = []
    all_conflicts = []
    all_user_stats = defaultdict(lambda: {'count': 0, 'total_score': 0, 'positive': 0, 'negative': 0})
    
    try:
        async with TelegramClient('session', api_id, api_hash) as client:
            await client.start(phone)
            
            for channel in channels:
                if flag.exit():
                    break
                    
                print(f"\n–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–∞–Ω–∞–ª: {channel}")
                start_time = time.time()
                
                results, conflicts, user_stats = await analyze_channel_async(client, channel)
                all_results.extend(results)
                all_conflicts.extend(conflicts)
                
                for user, stats in user_stats.items():
                    for key in ['count', 'total_score', 'positive', 'negative']:
                        all_user_stats[user][key] += stats[key]
                
                print(f"–ö–∞–Ω–∞–ª {channel} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {time.time()-start_time:.1f} —Å–µ–∫")
                
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
    
    return all_results, all_user_stats

def analyze_messages():
    try:
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(analyze_messages_async())
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ –≥–ª–∞–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {str(e)}")
        return [], defaultdict(lambda: {'count': 0, 'total_score': 0, 'positive': 0, 'negative': 0})

# –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (create_interactive_plot, print_top_messages –∏ —Ç.–¥.) –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

def main():
    try:
        query = input("–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–æ—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π): ").strip().lower()

        print("–ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")
        start_time = time.time()
        
        messages, user_stats = analyze_messages()
        df = pd.DataFrame(messages)

        if query:
            word_forms = get_word_forms(query)
            pattern = '|'.join(re.escape(wf) for wf in word_forms)
            df = df[df['–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç'].str.contains(pattern, case=False, na=False)]

        if len(df) > 0:
            print(f"\n–ù–∞–π–¥–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(df)}")
            print(f"–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {time.time()-start_time:.1f} —Å–µ–∫")
            
            # –í—ã–≤–æ–¥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            print_distribution(df)
            
            # –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π
            print("\n–ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π:")
            for category in ["–ù–µ–≥–∞—Ç–∏–≤–Ω–æ", "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ", "–ü–æ–∑–∏—Ç–∏–≤–Ω–æ"]:
                examples = df[df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] == category].head(1)
                if not examples.empty:
                    print(f"\n{category}:")
                    print(examples[['–î–∞—Ç–∞', '–ö–∞–Ω–∞–ª', '–¢–µ–∫—Å—Ç', '–û–±—â–∏–π –±–∞–ª–ª']].to_string(index=False))
            
            # –¢–æ–ø —Å–æ–æ–±—â–µ–Ω–∏–π
            print_top_messages(df, "–ü–æ–∑–∏—Ç–∏–≤–Ω–æ")
            print_top_messages(df, "–ù–µ–≥–∞—Ç–∏–≤–Ω–æ")
            
            # –¢–æ–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            print_top_users(user_stats, "–ü–æ–∑–∏—Ç–∏–≤–Ω–æ")
            print_top_users(user_stats, "–ù–µ–≥–∞—Ç–∏–≤–Ω–æ")
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            create_interactive_plot(df, query)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            with open('results.json', 'w', encoding='utf-8') as f:
                json.dump(df.to_dict('records'), f, ensure_ascii=False, indent=2)
            
            df[['–î–∞—Ç–∞', '–ö–∞–Ω–∞–ª', '–¢–µ–∫—Å—Ç', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '–û–±—â–∏–π –±–∞–ª–ª', '–Ø–∑—ã–∫', '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å']].to_csv(
                'sentiment_report.csv', index=False, encoding='utf-8')
            print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª—ã: results.json –∏ sentiment_report.csv")
        else:
            print("–°–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É.")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ main: {str(e)}")
    finally:
        print("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω.")

if __name__ == "__main__":
    main()